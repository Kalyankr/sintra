# Sintra Project Report

**Date:** January 12, 2026  
**Version:** 0.1.0  
**Repository:** https://github.com/Kalyankr/sintra

---

## Executive Summary

**Sintra** is an autonomous AI agent that optimizes Large Language Models (LLMs) for edge hardware deployment. It uses an LLM "Architect" to iteratively propose compression strategies, benchmark them on target hardware, and converge on optimal configurations.

**Current Status:** ğŸŸ¡ **Functional Prototype** â€” Core workflow operational, real compression pipeline implemented but requires llama.cpp toolchain.

---

## Project Goal

> Automatically compress any HuggingFace LLM to run efficiently on resource-constrained edge devices (Raspberry Pi, Jetson, Apple Silicon) while meeting user-defined performance targets (TPS, accuracy, VRAM).

---

## What Has Been Completed

### Phase 0: Bug Fixes & Code Quality (Session 1)
| Task | Status | Details |
|------|--------|---------|
| Entry point fix | âœ… | Was printing "Hello" instead of running workflow |
| Pydantic model alignment | âœ… | Added missing fields for YAML parsing |
| Router logic fix | âœ… | Invalid return value `"continue"` â†’ `"architect"` |
| State type fixes | âœ… | Fixed `best_recipe` type mismatch |
| Error handling | âœ… | Added `ProfileLoadError`, `LLMConnectionError`, `MissingAPIKeyError` |
| Comprehensive crash prevention | âœ… | Try/catch for all failure modes |
| Test suite | âœ… | 80 tests covering all modules |
| Type annotations | âœ… | Full typing + `py.typed` marker |

### Phase 1: Real Compression Pipeline (Session 2)
| Component | Status | Description |
|-----------|--------|-------------|
| `compression/downloader.py` | âœ… | HuggingFace model download with caching |
| `compression/quantizer.py` | âœ… | GGUF conversion & quantization (Q2_K-Q8_0) |
| `compression/evaluator.py` | âœ… | Perplexity-based accuracy measurement |
| Worker integration | âœ… | Supports REAL and LEGACY modes |
| CLI flags | âœ… | `--model-id`, `--hf-token`, `--real-compression` |
| Dependencies | âœ… | Added `huggingface_hub`, `safetensors` |

---

## Current Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SINTRA WORKFLOW                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚  ARCHITECT   â”‚â”€â”€â”€â–¶â”‚  BENCHMARKER â”‚â”€â”€â”€â–¶â”‚    CRITIC    â”‚             â”‚
â”‚   â”‚   (LLM)      â”‚    â”‚    (Lab)     â”‚    â”‚   (Judge)    â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚          â–²                                       â”‚                      â”‚
â”‚          â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                      â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   REPORTER   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚           (retry)     â”‚  (Archivist) â”‚     (converged)                  â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       COMPRESSION PIPELINE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   HuggingFace Model ID                                                  â”‚
â”‚          â”‚                                                              â”‚
â”‚          â–¼                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ ModelDownloader  â”‚â”€â”€â”€â”€â–¶â”‚  GGUFQuantizer   â”‚â”€â”€â”€â”€â–¶â”‚ AccuracyEval   â”‚ â”‚
â”‚   â”‚                  â”‚     â”‚                  â”‚     â”‚                â”‚ â”‚
â”‚   â”‚ â€¢ HF Hub API     â”‚     â”‚ â€¢ convert_hf_to_ â”‚     â”‚ â€¢ Perplexity   â”‚ â”‚
â”‚   â”‚ â€¢ Caching        â”‚     â”‚   gguf.py        â”‚     â”‚ â€¢ Quick tests  â”‚ â”‚
â”‚   â”‚ â€¢ Gated models   â”‚     â”‚ â€¢ llama-quantize â”‚     â”‚                â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â€¢ Q2_K â†’ Q8_0    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                         â”‚
â”‚   Cache: ~/.cache/sintra/{downloads,gguf,quantized}/                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
sintra/
â”œâ”€â”€ pyproject.toml              # Dependencies, build config
â”œâ”€â”€ profiles/                   # Hardware target definitions
â”‚   â”œâ”€â”€ raspberry_pi_5.yaml
â”‚   â””â”€â”€ mac_mini_m4.yaml
â”œâ”€â”€ samples/
â”‚   â””â”€â”€ example_output.json     # Sample optimization result
â”œâ”€â”€ src/sintra/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # Entry point, workflow execution
â”‚   â”œâ”€â”€ cli.py                  # Argument parsing
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ factory.py          # LLM provider factory
â”‚   â”‚   â”œâ”€â”€ nodes.py            # Workflow nodes (architect, benchmarker, critic, reporter)
â”‚   â”‚   â”œâ”€â”€ state.py            # TypedDict state definition
â”‚   â”‚   â””â”€â”€ utils.py            # History formatting
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â”‚   â”œâ”€â”€ executor.py         # Mock and Standalone executors
â”‚   â”‚   â””â”€â”€ worker/
â”‚   â”‚       â””â”€â”€ runner.py       # Subprocess worker for benchmarking
â”‚   â”œâ”€â”€ compression/            # NEW - Real compression pipeline
â”‚   â”‚   â”œâ”€â”€ downloader.py       # HuggingFace model download
â”‚   â”‚   â”œâ”€â”€ quantizer.py        # GGUF quantization
â”‚   â”‚   â””â”€â”€ evaluator.py        # Accuracy measurement
â”‚   â”œâ”€â”€ profiles/
â”‚   â”‚   â”œâ”€â”€ models.py           # Pydantic models
â”‚   â”‚   â””â”€â”€ parser.py           # YAML profile loader
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ console.py          # Rich console output
â””â”€â”€ tests/                      # 80 tests
    â”œâ”€â”€ test_compression.py
    â”œâ”€â”€ test_executor.py
    â”œâ”€â”€ test_models.py
    â”œâ”€â”€ test_nodes.py
    â”œâ”€â”€ test_parser.py
    â”œâ”€â”€ test_state.py
    â””â”€â”€ test_utils.py
```

---

## How to Use

### Basic Usage (Debug Mode - No LLM Required)
```bash
uv run sintra profiles/raspberry_pi_5.yaml --debug
```

### With LLM Architect
```bash
# Ollama (local)
ollama serve  # Start Ollama first
uv run sintra profiles/raspberry_pi_5.yaml

# OpenAI
export OPENAI_API_KEY=sk-...
uv run sintra profiles/raspberry_pi_5.yaml --provider openai --model gpt-4o

# Anthropic
export ANTHROPIC_API_KEY=...
uv run sintra profiles/raspberry_pi_5.yaml --provider anthropic --model claude-3-5-sonnet-latest
```

### Real Compression Mode (Requires llama.cpp)
```bash
# Install llama.cpp first
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make llama-quantize

# Run with real compression
uv run sintra profiles/raspberry_pi_5.yaml \
    --real-compression \
    --model-id TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
    --provider openai --model gpt-4o
```

---

## What Still Needs Implementation

### Phase 2: Pruning & Layer Dropping (Priority: High)
| Task | Effort | Description |
|------|--------|-------------|
| Pre-conversion layer removal | 3 days | Remove transformer layers before GGUF conversion |
| `pruning_ratio` implementation | 2 days | Apply structured pruning to attention/FFN weights |
| Integration with Architect prompts | 1 day | Make Architect aware of pruning effects |

**Why needed:** Currently `pruning_ratio` and `layers_to_drop` in ModelRecipe are ignored.

### Phase 3: Advanced Accuracy Evaluation (Priority: Medium)
| Task | Effort | Description |
|------|--------|-------------|
| Full perplexity calculation | 2 days | Proper log-likelihood computation |
| MMLU subset evaluation | 2 days | Standard benchmark for reasoning |
| Custom task evaluation | 1 day | User-defined eval datasets |

**Why needed:** Current accuracy is estimated heuristically, not measured properly.

### Phase 4: Production Features (Priority: Low)
| Task | Effort | Description |
|------|--------|-------------|
| Docker isolation | 3 days | Sandboxed worker execution with resource limits |
| Multi-objective optimization | 5 days | Pareto frontier for TPS/accuracy tradeoffs |
| Model export | 2 days | Save optimized model to disk (not just recipe JSON) |
| CI/CD pipeline | 1 day | GitHub Actions for tests, linting, releases |
| PyPI publishing | 1 day | `pip install sintra` |

### Future Enhancements (Ideas)
| Enhancement | Description |
|-------------|-------------|
| **Auto-discovery mode** | "Find the best possible accuracy within my hardware limits" â€” no accuracy floor required. The agent explores the Pareto frontier and returns the optimal accuracy achievable given VRAM/TPS constraints. |
| **Presets for common scenarios** | Built-in profiles like `--preset chatbot` (high TPS, moderate accuracy) or `--preset batch` (low TPS, high accuracy) for users who don't want to define custom targets. |
| **Target guardrails** | Warn users if targets are unrealistic (e.g., 100 TPS on Raspberry Pi) before running the optimization loop. |

---

## Quantization Support Matrix

| Bits | Type | Size Reduction | Quality | Implemented |
|------|------|----------------|---------|-------------|
| 2 | Q2_K | ~87% | Poor | âœ… |
| 3 | Q3_K_M | ~81% | Fair | âœ… |
| 4 | Q4_K_M | ~75% | Good | âœ… (recommended) |
| 5 | Q5_K_M | ~69% | Very Good | âœ… |
| 6 | Q6_K | ~62% | Excellent | âœ… |
| 8 | Q8_0 | ~50% | Near-FP16 | âœ… |

---

## Test Coverage

```
tests/test_compression.py    21 tests  âœ…
tests/test_executor.py        8 tests  âœ…
tests/test_models.py         17 tests  âœ…
tests/test_nodes.py          18 tests  âœ…
tests/test_parser.py          8 tests  âœ…
tests/test_state.py           3 tests  âœ…
tests/test_utils.py           5 tests  âœ…
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL                        80 tests  âœ…
```

---

## Dependencies

### Runtime
- `langgraph` - Agent workflow orchestration
- `langchain-*` - LLM provider integrations (OpenAI, Anthropic, Google, Ollama)
- `llama-cpp-python` - GGUF model loading and inference
- `huggingface_hub` - Model downloading
- `pydantic` - Data validation
- `rich` - Console UI
- `python-dotenv` - Environment variable management

### External (for real compression)
- `llama.cpp` - GGUF conversion and quantization binaries

---

## Git Branches

| Branch | Status | Description |
|--------|--------|-------------|
| `main` | âœ… Stable | Bug fixes, error handling, tests |
| `feature/real-compression` | âœ… Ready for merge | Compression pipeline implementation |

---

## Summary

| Aspect | Status |
|--------|--------|
| **Core Workflow** | âœ… Fully functional |
| **LLM Integration** | âœ… OpenAI, Anthropic, Google, Ollama |
| **Error Handling** | âœ… Comprehensive |
| **Test Coverage** | âœ… 80 tests passing |
| **Model Download** | âœ… HuggingFace Hub with caching |
| **Quantization** | âœ… Q2_K through Q8_0 |
| **Accuracy Eval** | ğŸŸ¡ Basic (needs full perplexity) |
| **Pruning** | âŒ Not implemented |
| **Layer Dropping** | âŒ Not implemented |
| **Docker Isolation** | âŒ Not implemented |

---

## Estimated Remaining Work

| Phase | Effort | Priority |
|-------|--------|----------|
| Phase 2: Pruning/Layer Dropping | ~6 days | ğŸ”´ High |
| Phase 3: Accuracy Evaluation | ~5 days | ğŸŸ¡ Medium |
| Phase 4: Production Features | ~12 days | ğŸŸ¢ Low |
| **Total** | **~23 days** | |

**Recommended Next Step:** Merge `feature/real-compression` to `main`, then implement pruning/layer dropping (Phase 2).

---

*Generated: January 12, 2026*
